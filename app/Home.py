"""
CardLens Research Terminal â€” Home / Control Panel
"""

from __future__ import annotations

import os
import sys
from pathlib import Path

# Allow imports from project root
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv

load_dotenv()

import streamlit as st

# Load secrets from Streamlit Cloud secrets if available (overrides .env)
try:
    for _key in ["OPENAI_API_KEY", "DEFAULT_TICKER"]:
        if _key in st.secrets:
            os.environ[_key] = st.secrets[_key]
except Exception:
    pass  # No secrets file â€” fall back to .env

st.set_page_config(
    page_title="CardLens Research Terminal",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ Sidebar controls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.title("ğŸ” CardLens")
    st.caption("MGMT690 Â· Project 2 Â· Mastercard Case")
    st.divider()

    st.markdown("**ğŸŒ Fetch & Index Documents**")
    st.caption("Downloads case press releases + 10-K, then builds the AI search index.")

    if st.button("ğŸŒ Fetch & Index Documents", use_container_width=True):
        with st.spinner("Fetching case documents from the webâ€¦"):
            try:
                from src.fetch_docs import fetch_all

                results = fetch_all()
                ok = [r for r in results if r["status"] == "ok"]
                fail = [r for r in results if r["status"] != "ok"]
                if ok:
                    st.success(f"Fetched {len(ok)} document(s).")
                if fail:
                    for r in fail:
                        st.warning(f"âš  {r['filename']}: {r.get('error', 'failed')}")
            except Exception as e:
                st.error(f"Fetch error: {e}")

        with st.spinner("Ingesting & chunking documentsâ€¦"):
            try:
                from src.ingest import ingest_all

                n_chunks = ingest_all()
            except Exception as e:
                st.error(f"Ingest error: {e}")
                n_chunks = 0

        with st.spinner("Building embeddings indexâ€¦"):
            try:
                from src.embeddings import build_index

                n_emb = build_index()
            except Exception as e:
                st.error(f"Embeddings error: {e}")
                n_emb = 0

        if n_emb == 0 and n_chunks == 0:
            st.warning("No documents indexed. Check fetch errors above.")
        else:
            st.success(f"âœ… Indexed {n_chunks} chunks Â· {n_emb} embeddings built. AI Chat is ready!")

    st.divider()

    st.markdown("**ğŸ“Š Settings**")
    horizon = st.selectbox("Analysis Horizon", ["1W", "1M", "3M", "6M"], index=2)
    st.session_state["horizon"] = horizon

    st.divider()
    st.caption(
        "**About**: CardLens is a case-grounded equity research terminal built for "
        "MGMT690 Project 2. All analysis is educational â€” not investment advice."
    )

# â”€â”€ Main content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ” CardLens Research Terminal")
st.subheader("MGMT690 Project 2 Â· Mastercard (MA) Equity Research")

st.markdown(
    """
Welcome to **CardLens Research Terminal** â€” a case-grounded intelligent equity research workbench
focused on **Mastercard (NYSE: MA)** and two landmark case events:

> ğŸ¤– **Agent Suite** (Jan 2026) â€” Mastercard's infrastructure for AI-native agentic commerce
> ğŸ”’ **Cloudflare Partnership** (Feb 2026) â€” Comprehensive cyber-defense for agentic payments
"""
)

st.divider()

# â”€â”€ Getting started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸš€ Getting Started")

col_gs1, col_gs2 = st.columns([1, 2])
with col_gs1:
    st.markdown(
        """
**Two steps to unlock AI Chat:**

1. Click **ğŸŒ Fetch & Index Documents** in the sidebar
2. Navigate to **ğŸ’¬ AI Research Chat** and ask anything

That's it â€” no manual file uploads needed.
        """
    )
with col_gs2:
    st.info(
        "The fetch step downloads four case documents directly from the web "
        "(Mastercard newsroom, Cloudflare press releases, SEC EDGAR 10-K) "
        "and builds a semantic search index. "
        "Requires an **OpenAI API key** in your environment or Streamlit secrets.",
        icon="ğŸ’¡",
    )

st.divider()

# â”€â”€ Page map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ“‹ Research Pages")

st.markdown(
    """
| # | Page | What it covers |
|---|------|----------------|
| 1 | **ğŸ“„ Case Overview** | Agent Suite + Cloudflare thesis, event timeline, key analytical questions |
| 2 | **ğŸ“Š Fundamentals** | Revenue, margins, ROE, FCF, quality checklist â€” cited to 10-K |
| 3 | **ğŸ“ˆ Technicals** | Candlestick chart, SMA 20/50/200, RSI, MACD, rolling volatility, max drawdown |
| 4 | **ğŸ’° Valuation** | DCF-lite with sliders, WACC Ã— terminal-growth sensitivity table, peer comps |
| 5 | **ğŸ“° News** | Pinned case press releases + live Mastercard newsroom RSS + Google News |
| 6 | **ğŸ’¬ AI Research Chat** | Ask anything â€” answers grounded in case documents with citations |
| 7 | **â­ Decision** | Buy / Hold / Avoid signal, data-derived horizon return ranges |
"""
)

st.divider()

# â”€â”€ Index status dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ“¦ Document Index Status")

PROJECT_ROOT = Path(__file__).parent.parent
index_file = PROJECT_ROOT / "data" / "index" / "embeddings.npz"
raw_dir = PROJECT_ROOT / "data" / "raw"
raw_files = [f for f in raw_dir.iterdir() if f.suffix.lower() in {".pdf", ".txt", ".md"}] if raw_dir.exists() else []

s1, s2, s3 = st.columns(3)
s1.metric("Documents in data/raw", len(raw_files))
s2.metric("Search Index", "âœ… Ready" if index_file.exists() else "âŒ Not built")
s3.metric("Ticker", "MA Â· Mastercard")

if raw_files:
    with st.expander("View indexed documents"):
        for f in sorted(raw_files):
            size_kb = f.stat().st_size / 1024
            st.markdown(f"- `{f.name}` â€” {size_kb:.1f} KB")
else:
    st.caption("No documents yet â€” click **ğŸŒ Fetch & Index Documents** in the sidebar.")
